name: 'Deploy Docker to AWS (EC2)'
description: 'Deploy a Docker app to an AWS Virtual Machine (EC2) with Docker Compose'
branding:
  icon: upload-cloud
  color: red
inputs:
  # Checkout
  checkout:
    description: 'Specifies if this action should checkout the code'
    required: false
    default: 'true'
  bitops_code_only:
    description: 'Will run only the generation phase of BitOps, where the Terraform and Ansible code is built.'
    required: false
  bitops_code_store:
    description: 'Store BitOps code as a GitHub artifact'
    required: false
    default: false
  
  # GitHub Deployment repo inputs
  gh_deployment_input_terraform:
    description: 'Folder to store Terraform files to be included during Terraform execution.'
    required: false
  gh_deployment_input_ansible:
    description: 'Folder where a whole Ansible structure is expected. If missing bitops.config.yaml a default will be generated.'
    required: false
  gh_deployment_input_ansible_playbook:
    description: 'Main playbook to be looked for.'
    required: false
    default: playbook.yml
  gh_deployment_input_ansible_extra_vars_file:
    description: "Relative path to file from project root to Ansible vars file to be applied. "
    required: false
  gh_deployment_action_input_ansible_extra_vars_file:
    description: "Relative path to file from project root to Ansible vars file to be applied into the Action Ansible execution. "
    required: false

  # AWS
  aws_access_key_id:
    description: 'AWS access key ID'
    required: true
  aws_secret_access_key:
    description: 'AWS secret access key'
    required: true
  aws_session_token:
    description: 'AWS session token'
    required: false
  aws_default_region:
    description: 'AWS default region'
    required: false
    default: us-east-1
  tf_state_bucket:
    description: 'AWS S3 bucket to use for Terraform state. Defaults to `${org}-${repo}-{branch}-tf-state`'
    required: false
  tf_state_bucket_destroy:
    description: 'Force purge and deletion of S3 bucket defined. Any file contained there will be destroyed. `stack_destroy` must also be `true`'
    required: false
  aws_resource_identifier:
    description: 'Set to override the AWS resource identifier for the deployment.  Defaults to `${org}-{repo}-{branch}`.  Use with destroy to destroy specific resources.'
    required: false
  
  # ENV files
  repo_env:
    description: 'File containing environment variables to be used with the app'
    required: false
    default: 'repo_env'
  dot_env:
    description: '`.env` file to be used with the app from Github secrets'
    required: false
  ghv_env:
    description: '`.env` file to be used with the app from Github variables'
    required: false
  aws_secret_env:
    description: 'Secret name to pull env variables from AWS Secret Manager'
    required: false
  
  # Application
  docker_remove_orphans:
    description: 'Toggle --remove-orphans flag. Defaults to false.'
    required: false
  docker_full_cleanup:
    description: 'Set to true to run docker-compose down and docker system prune --all --force --volumes after.'
    required: false
  app_directory:
    description: 'Relative path for the directory of the app (i.e. where `Dockerfile` and `docker-compose.yaml` files are located). This is the directory that is copied to the EC2 instance.  Default is the root of the repo. Add a .gha-ignore file with a list of files to be exluded.'
    required: false
  app_directory_cleanup:
    description: 'Will generate a timestamped compressed file and delete the app repo directory.'
    required: false
  app_port:
    description: 'Port to expose for the app'
    required: false
    default: 3000
  lb_port:
    description: 'Load balancer listening port. Defaults to 80 if NO FQDN provided, 443 if FQDN provided'
    required: false
  lb_healthcheck: 
    description: 'Load balancer health check string. Defaults to HTTP:app_port'
    required: false
  
  # EC2 Instance
  ec2_instance_profile:
    description: 'The AWS IAM instance profile to use for the EC2 instance'
    required: false
  ec2_instance_type: 
    description: 'The AWS Instance type'
    required: false
  ec2_ami_id:
    description: 'AWS AMI ID. Will default to the latest Ubuntu 22.04 server image (HVM) '
    required: false
  ec2_ami_update:
    description: 'Set this to true if you want to recreate the EC2 instance if there is a newer version of the AMI.'
    required: false
  ec2_volume_size: 
    description: 'The size of the volume (in GB) on the AWS Instance'
    required: false
    default: "8"
  ec2_root_preserve:
    description: 'Set this to true to avoid deletion of root volume on termination. Defaults to false.'
    required: false
  ec2_user_data_file: 
    description: 'Relative path in the repo for a user provided script to be executed with Terraform EC2 Instance creation.'
    required: false
  ec2_user_data_replace_on_change:
    description: 'If user_data file changes, instance will stop and start. Hence public IP will change. Defaults to true.'
    required: false

  # AWS VPC Inputs
  aws_vpc_create:
    description: 'Define if a VPC should be created'
    required: false
  aws_vpc_name:
    description: 'Set a specific name for the VPC'
    required: false
  aws_vpc_cidr_block:
    description: 'Define Base CIDR block which is divided into subnet CIDR blocks. Defaults to 10.0.0.0/16.'
    required: false
  aws_vpc_public_subnets:
    description: 'Comma separated list of public subnets. Defaults to 10.10.110.0/24'
    required: false
  aws_vpc_private_subnets:
    description: 'Comma separated list of private subnets. If none, none will be created.'
    required: false
  aws_vpc_availability_zones:
    description: 'Comma separated list of availability zones. Defaults to `aws_default_region.'
    required: false
  aws_vpc_id:
    description: 'AWS VPC ID. Accepts `vpc-###` values.'
    required: false
  aws_vpc_subnet_id:
    description: 'Specify a Subnet to be used with the instance. If none provided, will pick one.'
    required: false

  # AWS EFS
  aws_efs_create:
    description: 'Toggle to indicate whether to create and EFS and mount it to the ec2 as a part of the provisioning. Note: The EFS will be managed by the stack and will be destroyed along with the stack'
    required: false
  aws_efs_create_ha:
    description: 'Toggle to indicate whether the EFS resource should be highly available (target mounts in all available zones within region)'
    required: false
  aws_efs_fs_id:
    description: 'ID of existing EFS'
    required: false
  aws_efs_vpc_id:
    description: 'ID of the VPC for the EFS mount target. If aws_efs_create_ha is set to true, will create one mount target per subnet available in the VPC. If not, will pick one.'
    required: false
  aws_efs_subnet_ids:
    description: 'ID or IDs of the subnet for the EFS mount target.'
  aws_efs_security_group_name:
    description:  'The name of the EFS security group'
    required: false
  aws_efs_create_replica:
    description: 'Toggle to indiciate whether a read-only replica should be created for the EFS primary file system'
    required: false
  aws_efs_replication_destination:
    description: 'AWS Region to target for replication'
    required: false
  aws_efs_enable_backup_policy:
    description: 'Toggle to indiciate whether the EFS should have a backup policy, default is false'
    required: false
  aws_efs_transition_to_inactive:
    description: 'Indicates how long it takes to transition files to the IA storage class.'
    required: false
  aws_efs_mount_target: 
    description: 'Directory path in the EFS volume to mount directory to. Default is /.'
    required: false
  aws_efs_ec2_mount_point:
    description: 'Directory path in EC2 Instance to mount the EFS volume. Default is `data`. Exported as `HOST_DIR` in `.env`'
    required: false
  
  # Stack management
  stack_destroy:
    description: 'Set to "true" to Destroy the stack. Will delete the elb_logs bucket after the destroy action runs.'
    required: false
  
  # Domains
  domain_name:
    description: 'Define the root domain name for the application. e.g. app.com'
    required: false
  sub_domain:
    description: 'Define the sub-domain part of the URL. Defaults to `${org}-${repo}-{branch}`'
    required: false
  root_domain:
    description: 'Deploy to root domain. Will generate two DNS recrods, one for root, another for www'
    required: false
  cert_arn:
    description: 'Define the certificate ARN to use for the application'
    required: false
  create_root_cert:
    description: 'Generates and manage the root cert for the application'
    required: false
  create_sub_cert: 
    description: 'Generates and manage the sub-domain certificate for the application'
    required: false
  no_cert:
    description: 'Makes the application not to use a certificate by disabling certificate lookup.'
    required: false
    
  # Terraform
  targets:
    description: 'A list of targets to create before the full stack creation. Example: `'
    required: false
  additional_tags:
    description: 'A JSON object of additional tags that will be included on created resources. Example: `{"key1": "value1", "key2": "value2"}`'
    required: false
   
  # Secret Manager
  create_keypair_sm_entry:
    description: "Generates and manages a secret manager entry that contains the public and private keys created for the ec2 instance."
    required: false
    
  # AWS RDS
  aws_aurora_enable:
    description: 'Set to "true" to enable a postgres database'
    required: false
  aws_aurora_engine: 
    description: 'Which Database engine to use'
    required: false
  aws_aurora_engine_version:
    description: 'Specify Postgres version'
    required: false
  aws_aurora_database_group_family:
    description: 'Postgres database group family'
    required: false 
  aws_aurora_instance_class:
    description: 'Define the size of the instances in the DB cluster'
    required: false
  aws_aurora_security_group_name:
    description:  'The name of the Postgres security group'
    required: false
  aws_aurora_subnets:
    description: 'Specify which subnets to use as a list of strings.  Example: `i-1234,i-5678,i-9101`'
    required: false
  aws_aurora_cluster_name:
    description: 'Specify a cluster name. Will be created if it does not exist'
    required: false
  aws_aurora_database_name:
    description: 'Specify a database name. Will be created if it does not exist'
    required: false
  aws_aurora_database_port:
    description: 'Postgres database port'
    required: false
  aws_aurora_restore_snapshot:
    description: 'Restore a snapshot to the DB. Should be used only once. Changes in this value will destroy and recreate the database completely.'
    required: false
  aws_aurora_snapshot_name:
    description: 'Takes a snapshot of the cluster using that name. If none definded, no snapshot will be made. If snap already exists, no new one will be created.'
    required: false
  aws_aurora_snapshot_overwrite:
    description: 'If the snapshot name is the same as an existing one, will destroy and create a new one.'
    required: false
  aws_aurora_database_protection:
    description: 'Protects the database from deletion.'
    required: false
  aws_aurora_database_final_snapshot:
    description: 'Generates a snapshot of the database before deletion.'
    required: false

  # Ansible
  application_mount_target:
    description: "Directory path in application env to mount directory, default is `data`"
    required: false
    default: data
  data_mount_target:
    description: "Directory path within docker env to mount directory to, default is `/data`"
    required: false
  efs_mount_target:
    description: "Directory path in efs to mount directory to, default is `/`"
    required: false

outputs:
  vm_url:
    description: "The URL of the generated app"
    value: ${{ steps.deploy.outputs.vm_url }}

runs:
  using: 'composite'
  steps:
    - name: Checkout if required
      if: ${{ inputs.checkout == 'true' }}
      uses: actions/checkout@v3

    - name: Invert boolean Variable
      shell: bash
      id: set-cert
      if: ${{ inputs.no_cert != 'true' }}
      run: echo "enable_cert=true" >> $GITHUB_OUTPUT

    - name: Set healtcheck default
      shell: bash
      id: set-healthcheck
      if: ${{ inputs.lb_healthcheck == '' }}
      run: echo "lb_healthcheck=HTTP:${{ inputs.app_port }}/" >> $GITHUB_OUTPUT
      
    - name: Deploy with BitOps
      id: deploy
      uses: bitovi/github-actions-commons@v0.0.3
      with:
        # Current repo vars
        gh_action_repo: ${{ github.action_path }}

        bitops_code_only: ${{ inputs.bitops_code_only }}
        bitops_code_store: ${{ inputs.bitops_code_store }}

        # Deployment repo
        gh_deployment_input_terraform: ${{ inputs.gh_deployment_input_terraform }}
        gh_deployment_input_ansible: ${{ inputs.gh_deployment_input_ansible }}
        gh_deployment_input_ansible_playbook: ${{ inputs.gh_deployment_input_ansible_playbook }}
        gh_deployment_input_ansible_extra_vars_file: ${{ inputs.gh_deployment_input_ansible_extra_vars_file }}
        gh_deployment_action_input_ansible_extra_vars_file: ${{ inputs.gh_deployment_action_input_ansible_extra_vars_file }}
        
        # Action repo
        gh_action_input_terraform: ${{ inputs.gh_action_input_terraform }}
        gh_action_input_ansible: ${{ inputs.gh_action_input_ansible }}
        gh_action_input_ansible_playbook: ${{ inputs.gh_action_input_ansible_playbook }}


        # Action main inputs
        checkout: ${{ inputs.checkout }}
        tf_stack_destroy: ${{ inputs.stack_destroy }}
        tf_state_bucket: ${{ inputs.tf_state_bucket }}
        tf_state_bucket_destroy: ${{ inputs.tf_state_bucket_destroy }}
        tf_state_bucket_provider: 'aws'
        tf_targets: ${{ inputs.targets }}


        # AWS
        aws_access_key_id: ${{ inputs.aws_access_key_id }}
        aws_secret_access_key: ${{ inputs.aws_secret_access_key }}
        aws_session_token: ${{ inputs.aws_session_token }}
        aws_default_region: ${{ inputs.aws_default_region }}
        aws_resource_identifier: ${{ inputs.aws_resource_identifier }}
        aws_additional_tags: ${{ inputs.additional_tags }}

        # ENV
        env_aws_secret: ${{ inputs.aws_secret_env }}
        env_repo: ${{ inputs.repo_env }}
        env_ghs: ${{ inputs.dot_env }}
        env_ghv: ${{ inputs.ghv_env }}

        # EC2
        aws_ec2_instance_create: true
        aws_ec2_ami_id: ${{ inputs.ec2_ami_id }}
        aws_ec2_ami_update: ${{ inputs.ec2_ami_update }}
        aws_ec2_iam_instance_profile: ${{ inputs.ec2_instance_profile }}
        aws_ec2_instance_type: ${{ inputs.ec2_instance_type }}
        aws_ec2_instance_root_vol_size: ${{ inputs.ec2_volume_size }}
        aws_ec2_instance_root_vol_preserve: ${{ inputs.ec2_root_preserve }}
        aws_ec2_create_keypair_sm: ${{ inputs.create_keypair_sm_entry }}
        aws_ec2_instance_public_ip: true
        aws_ec2_user_data_file: ${{ inputs.ec2_user_data_file }}
        aws_ec2_user_data_replace_on_change: ${{ inputs.ec2_user_data_replace_on_change }}

        ## AWS VPC
        aws_vpc_create: ${{ inputs.aws_vpc_create }}
        aws_vpc_name: ${{ inputs.aws_vpc_name }}
        aws_vpc_cidr_block: ${{ inputs.aws_vpc_cidr_block }}
        aws_vpc_public_subnets: ${{ inputs.aws_vpc_public_subnets }}
        aws_vpc_private_subnets: ${{ inputs.aws_vpc_private_subnets }}
        aws_vpc_availability_zones: ${{ inputs.aws_vpc_availability_zones }}
        aws_vpc_id: ${{ inputs.aws_vpc_id }}
        aws_vpc_subnet_id: ${{ inputs.aws_vpc_subnet_id }}

        # AWS Route53 Domains abd Certificates
        aws_r53_enable: true
        aws_r53_domain_name: ${{ inputs.domain_name }}
        aws_r53_sub_domain_name: ${{ inputs.sub_domain }}
        aws_r53_root_domain_deploy: ${{ inputs.root_domain }}
        aws_r53_enable_cert: ${{ steps.set-cert.outputs.enable_cert }}
        aws_r53_cert_arn: ${{ inputs.cert_arn }}
        aws_r53_create_root_cert: ${{ inputs.create_root_cert }}
        aws_r53_create_sub_cert: ${{ inputs.create_sub_cert }}

        # AWS ELB
        aws_elb_create: true
        aws_elb_app_port: ${{ inputs.app_port }}
        aws_elb_listen_port: ${{ inputs.lb_port }}
        aws_elb_healthcheck: ${{ steps.set-healthcheck.outputs.lb_healthcheck }}

        # AWS EFS
        aws_efs_create: ${{ inputs.aws_efs_create }}
        aws_efs_create_ha: ${{ inputs.aws_efs_create_ha }}
        aws_efs_fs_id: ${{ inputs.aws_efs_fs_id }}
        aws_efs_vpc_id: ${{ inputs.aws_efs_vpc_id }}
        aws_efs_subnet_ids: ${{ inputs.aws_efs_subnet_ids }}
        aws_efs_security_group_name: ${{ inputs.aws_efs_security_group_name }}
        aws_efs_create_replica: ${{ inputs.aws_efs_create_replica }}
        aws_efs_replication_destination: ${{ inputs.aws_efs_replication_destination }}
        aws_efs_enable_backup_policy: ${{ inputs.aws_efs_enable_backup_policy }}
        aws_efs_transition_to_inactive: ${{ inputs.aws_efs_transition_to_inactive }}
        aws_efs_mount_target: ${{ inputs.aws_efs_mount_target }}
        aws_efs_ec2_mount_point: ${{ inputs.aws_efs_ec2_mount_point }}

        # AWS AURORA
        aws_aurora_enable: ${{ inputs.aws_aurora_enable }}
        aws_aurora_engine:  ${{ inputs.aws_aurora_engine }}
        aws_aurora_engine_version:  ${{ inputs.aws_aurora_engine_version }}
        aws_aurora_database_group_family:  ${{ inputs.aws_aurora_database_group_family }}
        aws_aurora_instance_class: ${{ inputs.aws_aurora_instance_class }}
        aws_aurora_security_group_name: ${{ inputs.aws_aurora_security_group_name }}
        aws_aurora_subnets: ${{ inputs.aws_aurora_subnets }}
        aws_aurora_cluster_name: ${{ inputs.aws_aurora_cluster_name }}
        aws_aurora_database_name: ${{ inputs.aws_aurora_database_name }}
        aws_aurora_database_port: ${{ inputs.aws_aurora_database_port}}
        aws_aurora_restore_snapshot: ${{ inputs.aws_aurora_restore_snapshot }}
        aws_aurora_snapshot_name: ${{ inputs.aws_aurora_snapshot_name }}
        aws_aurora_snapshot_overwrite: ${{ inputs.aws_aurora_snapshot_overwrite }}
        aws_aurora_database_protection: ${{ inputs.aws_aurora_database_protection }}
        aws_aurora_database_final_snapshot: ${{ inputs.aws_aurora_database_final_snapshot }}

        # DOCKER
        docker_install: true
        docker_remove_orphans: ${{ inputs.docker_remove_orphans }}
        docker_full_cleanup: ${{ inputs.docker_full_cleanup }}
        docker_repo_app_directory: ${{ inputs.app_directory }}
        docker_repo_app_directory_cleanup: ${{ inputs.app_directory_cleanup }}
        docker_efs_mount_target: ${{ inputs.data_mount_target }}
